import logging
import os
from datetime import datetime

import pandas as pd
from airflow.decorators import dag, task
from airflow.providers.postgres.hooks.postgres import PostgresHook


@dag(
    dag_id="extract_postgres_marts",
    start_date=datetime(2025, 11, 17),
    schedule="55 2 * * *",
    catchup=False,
    tags=["gold"],
)
def extract_pipeline():
    logger = logging.getLogger("DAG: postgres")

    GOLD_DIR = "/usr/local/airflow/mylake/gold/"

    # ConexÃ£o com DW
    hook = PostgresHook(postgres_conn_id="postgres_dw")
    engine = hook.get_sqlalchemy_engine()

    def _dump_table(schema: str, table: str):
        """
        LÃª uma tabela do Postgres e salva em CSV no GOLD_DIR.
        Retorna o caminho do arquivo salvo (Ãºtil pra debug/log).
        """
        full_name = f"{schema}.{table}"
        logger.info(f"ğŸ“¤ Exportando {full_name} ...")

        # 1. Carrega tudo da tabela
        df = pd.read_sql(f"SELECT * FROM {full_name}", con=engine)

        # 2. Monta nome do arquivo com timestamp pra versionar
        # ts = pendulum.now("America/Sao_Paulo").strftime("%Y%m%d_%H%M%S")
        # filename = f"{table}_{ts}.csv"
        filename = f"{table}.csv"
        filepath = os.path.join(GOLD_DIR, filename)

        # 3. Salva CSV
        df.to_csv(filepath, sep=";", index=False)

        logger.info(f"âœ… Export concluÃ­do: {filepath}")
        return filepath

    #
    # Agora criamos UMA task por tabela que vocÃª quer exportar.
    # VocÃª pode ir adicionando/removendo tasks depois, bem fÃ¡cil.
    #

    @task(task_id="export_energia_clima")
    def export_export_energia_e_clima():
        return _dump_table(schema="marts", table="mrt_energia_clima")

    @task(task_id="export_energia_hora")
    def export_energia_hora():
        return _dump_table(schema="marts", table="mrt_energia_hora")

    @task(task_id="export_inflation")
    def export_inflation():
        return _dump_table(schema="marts", table="mrt_inflation")

    # Se vocÃª quiser mais tabelas, cria mais @task copiando esse padrÃ£o.

    # IMPORTANTE:
    # Aqui a gente simplesmente INSTANCIA as tasks.
    # NÃ£o vamos encadear com >> porque vocÃª pediu que elas sejam independentes.
    t1 = export_export_energia_e_clima()
    t2 = export_energia_hora()
    t3 = export_inflation()

    # Nenhuma dependÃªncia entre dep, vot, par.
    # Isso significa:
    # - Airflow pode rodar todas em paralelo quando vocÃª dÃ¡ trigger.
    # - Se uma falhar, as outras rodam normal.
    # - VocÃª pode atÃ© sÃ³ marcar "run" em uma task especÃ­fica pela UI se quiser gerar sÃ³ uma tabela.

    # opcional: vocÃª pode retornar algo aqui sÃ³ pra nÃ£o ficar "variÃ¡vel nÃ£o usada"
    return [t1, t2, t3]


dag = extract_pipeline()
